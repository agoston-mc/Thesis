NNEF Conformance Test Suite
===========================

This repository contains the NNEF confromance test suite, including the test cases themselves, scripts to generate the test cases and evaluate the results.


Design Philosophy
-----------------

Neural networks have applications in many domains, and it is expected that while certain neural network libraries are general purpose, applicable in many domains (such as image processing, audio processing, language processing), others would be specialized to certain domains only. The requirements and implementation details may differ substantially among domains. For example image processing requires 2 dimensional (spatial) data while language/audio processing works on 1 dimensional (temporal) data. Hence, to be able to assess the capabilities of specialized libraries, units tests must be separated by domain.

Furthermore, within a certain domain, such as image processing, neural networks may be used for various tasks (such as image classification, image segmentation, object detection). Although core networks may be similar among tasks, each task may have different evaluation criteria.

The methodology devised here is as follows. For each target task in a target domain, a set of unit tests are assembled to ensure coverage by a consumer library. These test cases are chosen such that fundamental neural network architectures in the domain are covered. Each unit test case consists of a neural network described in NNEF format (along with trained or random generated weights), and a set of input-output pairs (in NNEF tensor binary format). A consumer of NNEF must be able to compile the network, and execute it for the set of inputs, and must receive the proper output prescribed by the test case. The exact method of comparison of the actual and expected outputs depends on the task at hand (for example image classification or segmentation). For each task, a comparison metric is defined.

The tests consist of two kinds of test cases: _operation_ and _network_ level tests. Operation level tests cover operations one-by-one with various parameter combinations and are fed with random generated data. Network level tests cover popular networks from the literature, trained for a specific task on real world data.

Both kinds of tests contain a set of _core_ tests, and a set of _extension_ tests. In case of operation level tests, core tests contain test cases for operations in the core set of the NNEF specificaion (Appendix A). The extension tests contain tests for further operations included in the NNEF specificaion, or for extended parameterizations of operations that are in the core set with a limited parameter range. In case of network tests, the core set contains networks composed of only core operations (with limited parameter ranges). The extension tests contain extension operations.

The current focus is on the image processing domain, and the tasks of image classification and image segmentation. Further domains and tasks may be added in the future.


Operation Level Tests
---------------------

Operation level tests are generated from an NNEF source that describes the network structure (without data), usually consisting of some input nodes and a single operation. In most cases these test cases are written by hand to cover various parameter combinations. In some cases however, where a group of operations have the same interface (such as unary, binary and reduce operations), only their mathematical formula differs, the test sources are generated from a template. These generator scripts are located in the `scripts` folder.

Given the sources, the network data (random inputs and weights where necessary, and corresponding outputs) are generated by running the network through a backend. The backend is implemented as part of the NNEF-Tools package and relies on existing frameworks to execute the network. The generated networks are stored outside of this repository, as they contain significant amount of binary data. They can be downloaded using this [link](https://sfo2.digitaloceanspaces.com/nnef-public/cts/operations.zip).

To re-generate the models, the `generate_models.py` script can be used. It runs a sequence of tools from the `nnef_tools` package, which must be installed beforehand by running `python setup.py install` from the root of the NNEF-Tools repository (currently only available on the 'feature_pytorch-backend' branch). The generator tools in the `nnef_tools` can be used independently as well.


Network Level Tests
-------------------

Network level tests consist of trained networks converted to NNEF. They contain networks in two domains: _image classification_ and _image segmentation_. They can be downloaded using this [link](https://sfo2.digitaloceanspaces.com/nnef-public/cts/networks.zip).

The input data for network level tests consists of a set of images. These images are the same accross all networks for a given domain and hence are only stored once and can be downloaded here. Furthermore, separate images must be used for calibrating network quantization. These images can be downloaded from [here](https://sfo2.digitaloceanspaces.com/nnef-public/cts/classification-inputs.zip).


Creating a Submission
---------------------

Each test case consists of a network description and inputs to the network. A conforming implementation must provide the outputs of the given network for the given inputs. These outputs are then compared to _reference_ outputs (also included in the test cases), that were generated with a trusted framework implementation.

A submission must have the same folder structure as the test cases available from the above links, the must have a 'core' and 'extension' folder, each containing sub-folders for the individual test cases. Each test cases must have the appropriately named output tensors in it.

Given the system outputs and the reference outputs, the script named 'compare_outputs.py' must be run for both the operation level and network level tests. The script goes through all test cases, compares their results and prints a log. This log must also be included in the submission.
The script can be used as follows. It takes two path arguments; the first to the reference outputs folder, and the second to the outputs of the system under test. Each folder must contain a list of subfolders for the test cases. It must be run separately for the 'core' and 'extension' folders. When running for extensions, the `--extension` switch must be used to let the script know that not providing an output for a test case is not an error.

```
python compare_outputs.py path/to/reference/outputs/core path/to/system/outputs/core
python compare_outputs.py path/to/reference/outputs/extension path/to/system/outputs/extension --extension
```

Furthermore, the above must be executed for both operation level and network level tests. When running the comparison script for network tests, the comparison metric must also be specified with the `--metric` switch. The possible values are `relative-difference` (default), `classification` and `segmentation`. The script must be run separately for each test domain.


### Comparison Metrics

In what follows, the metrics used for comparison of reference outputs to outputs obtained from the system under test are listed. The reference outputs are not ground truth outputs, they are merely outputs generated with a trusted reference implementation with high precision (floating point) arithmetic. What is required is that the system under testing behaves like the reference system (as opposed to matching ground truth outputs in a given task).

A general purpose metric is used for comparing results of networks of single operations, while task specific network outputs are compared using a task specific metric. Essentially, these metrics have to compare two output tensors for their approximate equivalence. What 'approximate equivalence' means depends on the task, and the purpose of using task specific metrics is to allow for certain non-relevant discrepancies from the reference resulting from numeric approximations in the system under testing, while enforcing similarity to the reference in the relevant aspects.

##### General Purpose Metric

The general way to compare two tensors is to look at the maximum absolute value of the difference between each of their elements (i.e. max-norm of the flattened vector), and constrain that difference to be smaller than a threshold value. However, to know the proper scale of the threshold, the absolute difference needs to be normalized with the scale of the reference vector to get a relative difference. The mean of absolute values of the reference tensor is used as normalization term to avoid it being sensitive to outliers.

Suppose that the tensors are flattened to vectors _u_ (representing the output of the system under test) and _v_ (representing the reference value) of length _n_, the difference is defined as:

<img src="https://latex.codecogs.com/svg.latex?D(u,v)&space;=&space;\frac{\sqrt{\frac{1}{n}\sum_{i=1}^n&space;(u_i&space;-&space;v_i)^2}}{\sqrt{\frac{1}{n}\sum_{i=1}^n&space;v_i^2}}" title="D(u,v) = \frac{\sqrt{\frac{1}{n}\sum_{i=1}^n (u_i - v_i)^2}}{\sqrt{\frac{1}{n}\sum_{i=1}^n v_i^2}}" />

Then the acceptance criteria for two tensors being approximately equal is that D(u,v) < Î¸.

##### Classification Metric

In case of classification, the output is a vector of probabilities for each potential class (discrete probability distribution). We wish to measure how an implementation differs from a reference implementation (not how it scores against the ground truth labelling). An often used measure of how a distribution differs from a reference distribution the Kullbach-Leibler divergence. Given a distribution _p_, it's Kullbach-Leibler divergence from a reference distribution _q_ is defined as

<a href="https://www.codecogs.com/eqnedit.php?latex=D(p,q)&space;=&space;\sum_{i=1}^n&space;p_i&space;\log&space;\frac{p_i}{q_i}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?D(p,q)&space;=&space;\sum_{i=1}^n&space;p_i&space;\log&space;\frac{p_i}{q_i}" title="D(p,q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i}" /></a>

Then the acceptance criteria for two distributions being approximately equal is that D(u,v) < Î¸.

##### Segmentation Metric

Segmentation is similar to classification, except it is performed for every pixel in an image. In this case we don't compare all the class probabilities for each pixel. Instead, we simply measure what percentage of pixels is categorized the same way as the reference, leaving some room for approximations. The acceptance criteria for two segmentations being equal is that the number of mismatch pixels is smaller than a given percentage.